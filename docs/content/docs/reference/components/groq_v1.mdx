---
title: "Groq"
description: "The LPU Inference Engine by Groq is a hardware and software platform that delivers exceptional compute speed, quality, and energy efficiency."
---


Categories: Artificial Intelligence


Type: groq/v1

<hr />




## Connections

Version: 1


### Bearer Token

#### Properties

|      Name       |      Label     |     Type     |     Description     | Required |
|:---------------:|:--------------:|:------------:|:-------------------:|:--------:|
| token | Token | STRING |  | true |





<hr />


## Actions


### Ask
Name: ask

`Ask anything you want.`

#### Properties

|      Name       |      Label     |     Type     |     Description     | Required |
|:---------------:|:--------------:|:------------:|:-------------------:|:--------:|
| model | Model | STRING | ID of the model to use. | true |
| format | Format | STRING <details> <summary> Options </summary> <span title="User prompt and optional system prompt.">SIMPLE</span>, <span title="Full control over the messages sent to the model.">ADVANCED</span> </details> | Format of providing the prompt to the model. | true |
| userPrompt | Prompt | STRING | User prompt to the model. | true |
| systemPrompt | System Prompt | STRING | System prompt to the model. | false |
| attachments | Attachments | ARRAY <details> <summary> Items </summary> [FILE_ENTRY] </details> | Only text and image files are supported. Also, only certain models supports images. Please check the documentation. | false |
| messages | Messages | ARRAY <details> <summary> Items </summary> [&#123;STRING\(role), STRING\(content), [FILE_ENTRY]\(attachments)&#125;] </details> | A list of messages comprising the conversation so far. | true |
| response | Response | OBJECT <details> <summary> Properties </summary> &#123;STRING\(responseFormat), STRING\(responseSchema)&#125; </details> | The response from the API. | true |
| maxTokens | Max Tokens | INTEGER | The maximum number of tokens to generate in the chat completion. | false |
| n | Number of Chat Completion Choices | INTEGER | How many chat completion choices to generate for each input message. | false |
| temperature | Temperature | NUMBER | Controls randomness:  Higher values will make the output more random, while lower values like will make it more focused and deterministic. | false |
| topP | Top P | NUMBER | An alternative to sampling with temperature, called nucleus sampling,  where the model considers the results of the tokens with top_p probability mass. So 0.1 means only the tokens comprising the top 10% probability mass are considered. | false |
| frequencyPenalty | Frequency Penalty | NUMBER | Number between -2.0 and 2.0. Positive values penalize new tokens based on their existing frequency in the text so far, decreasing the model's likelihood to repeat the same line verbatim. | false |
| presencePenalty | Presence Penalty | NUMBER | Number between -2.0 and 2.0. Positive values penalize new tokens based on whether they appear in the text so far, increasing the model's likelihood to talk about new topics. | false |
| logitBias | Logit Bias | OBJECT <details> <summary> Properties </summary> &#123;&#125; </details> | Modify the likelihood of specified tokens appearing in the completion. | false |
| stop | Stop | ARRAY <details> <summary> Items </summary> [STRING] </details> | Up to 4 sequences where the API will stop generating further tokens. | false |
| user | User | STRING | A unique identifier representing your end-user, which can help admins to monitor and detect abuse. | false |

#### Example JSON Structure
```json
{
  "label" : "Ask",
  "name" : "ask",
  "parameters" : {
    "model" : "",
    "format" : "",
    "userPrompt" : "",
    "systemPrompt" : "",
    "attachments" : [ {
      "extension" : "",
      "mimeType" : "",
      "name" : "",
      "url" : ""
    } ],
    "messages" : [ {
      "role" : "",
      "content" : "",
      "attachments" : [ {
        "extension" : "",
        "mimeType" : "",
        "name" : "",
        "url" : ""
      } ]
    } ],
    "response" : {
      "responseFormat" : "",
      "responseSchema" : ""
    },
    "maxTokens" : 1,
    "n" : 1,
    "temperature" : 0.0,
    "topP" : 0.0,
    "frequencyPenalty" : 0.0,
    "presencePenalty" : 0.0,
    "logitBias" : { },
    "stop" : [ "" ],
    "user" : ""
  },
  "type" : "groq/v1/ask"
}
```

#### Output

The output for this action is dynamic and may vary depending on the input parameters. To determine the exact structure of the output, you need to execute the action.






