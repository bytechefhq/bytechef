{"categories":["ARTIFICIAL_INTELLIGENCE"],"customAction":null,"customActionHelp":null,"dataStreamItemReader":null,"dataStreamItemWriter":null,"description":"Get up and running with large language models.","icon":"path:assets/ollama.svg","tags":null,"metadata":null,"name":"ollama","resources":null,"version":1,"title":"Ollama","actions":[{"batch":null,"dynamicOutput":true,"deprecated":null,"description":"Ask anything you want.","help":null,"metadata":null,"name":"ask","properties":[{"advancedOption":null,"description":"ID of the model to use.","displayCondition":null,"expressionEnabled":null,"hidden":null,"metadata":{},"required":true,"name":"model","type":"STRING","defaultValue":null,"exampleValue":null,"label":"Model","placeholder":null,"controlType":"SELECT","languageId":null,"maxLength":null,"minLength":null,"options":[{"description":null,"label":"llama2-uncensored","value":"llama2-uncensored"},{"description":null,"label":"starling-lm","value":"starling-lm"},{"description":null,"label":"moondream","value":"moondream"},{"description":null,"label":"llava","value":"llava"},{"description":null,"label":"mistral-nemo","value":"mistral-nemo"},{"description":null,"label":"dolphin-phi","value":"dolphin-phi"},{"description":null,"label":"orca-mini","value":"orca-mini"},{"description":null,"label":"codellama","value":"codellama"},{"description":null,"label":"phi","value":"phi"},{"description":null,"label":"llama3.1","value":"llama3.1"},{"description":null,"label":"llama3","value":"llama3"},{"description":null,"label":"phi3","value":"phi3"},{"description":null,"label":"llama2","value":"llama2"},{"description":null,"label":"gemma","value":"gemma"},{"description":null,"label":"neural-chat","value":"neural-chat"},{"description":null,"label":"mistral","value":"mistral"}],"optionsDataSource":null},{"advancedOption":null,"description":"The format to return a response in.","displayCondition":null,"expressionEnabled":null,"hidden":null,"metadata":{},"required":null,"name":"format","type":"STRING","defaultValue":"json","exampleValue":null,"label":"Format","placeholder":null,"controlType":"SELECT","languageId":null,"maxLength":null,"minLength":null,"options":[{"description":null,"label":"JSON","value":"json"}],"optionsDataSource":null},{"advancedOption":null,"description":"A list of messages comprising the conversation so far.","displayCondition":null,"expressionEnabled":null,"hidden":null,"metadata":{},"required":true,"name":"messages","type":"ARRAY","defaultValue":null,"exampleValue":null,"label":"Messages","placeholder":null,"items":[{"advancedOption":null,"description":null,"displayCondition":null,"expressionEnabled":null,"hidden":null,"metadata":{},"required":null,"name":null,"type":"OBJECT","defaultValue":null,"exampleValue":null,"label":null,"placeholder":null,"additionalProperties":null,"multipleValues":null,"options":null,"properties":[{"advancedOption":null,"description":"The contents of the message.","displayCondition":null,"expressionEnabled":null,"hidden":null,"metadata":{},"required":true,"name":"content","type":"STRING","defaultValue":null,"exampleValue":null,"label":"Content","placeholder":null,"controlType":"TEXT","languageId":null,"maxLength":null,"minLength":null,"options":null,"optionsDataSource":null},{"advancedOption":null,"description":"The role of the messages author","displayCondition":null,"expressionEnabled":null,"hidden":null,"metadata":{},"required":true,"name":"role","type":"STRING","defaultValue":null,"exampleValue":null,"label":"Role","placeholder":null,"controlType":"SELECT","languageId":null,"maxLength":null,"minLength":null,"options":[{"description":null,"label":"system","value":"system"},{"description":null,"label":"user","value":"user"},{"description":null,"label":"assistant","value":"assistant"},{"description":null,"label":"tool","value":"tool"}],"optionsDataSource":null}],"optionsDataSource":null,"controlType":"OBJECT_BUILDER"}],"maxItems":null,"minItems":null,"multipleValues":null,"options":null,"optionsDataSource":null,"controlType":"ARRAY_BUILDER"},{"advancedOption":null,"description":"How many chat completion choices to generate for each input message.","displayCondition":null,"expressionEnabled":null,"hidden":null,"metadata":{},"required":false,"name":"n","type":"INTEGER","defaultValue":1,"exampleValue":null,"label":"Number of chat completion choices","placeholder":null,"maxValue":null,"minValue":null,"options":null,"optionsDataSource":null,"controlType":"INTEGER"},{"advancedOption":null,"description":"Controls randomness:  Higher values will make the output more random, while lower values like will make it more focused and deterministic.","displayCondition":null,"expressionEnabled":null,"hidden":null,"metadata":{},"required":false,"name":"temperature","type":"NUMBER","defaultValue":1.0,"exampleValue":null,"label":"Temperature","placeholder":null,"maxNumberPrecision":null,"maxValue":2.0,"minNumberPrecision":null,"minValue":0.0,"numberPrecision":null,"options":null,"optionsDataSource":null,"controlType":"NUMBER"},{"advancedOption":null,"description":"Up to 4 sequences where the API will stop generating further tokens.","displayCondition":null,"expressionEnabled":null,"hidden":null,"metadata":{},"required":false,"name":"stop","type":"ARRAY","defaultValue":null,"exampleValue":null,"label":"Stop","placeholder":null,"items":[{"advancedOption":null,"description":null,"displayCondition":null,"expressionEnabled":null,"hidden":null,"metadata":{},"required":null,"name":null,"type":"STRING","defaultValue":null,"exampleValue":null,"label":null,"placeholder":null,"controlType":"TEXT","languageId":null,"maxLength":null,"minLength":null,"options":null,"optionsDataSource":null}],"maxItems":null,"minItems":null,"multipleValues":null,"options":null,"optionsDataSource":null,"controlType":"ARRAY_BUILDER"},{"advancedOption":null,"description":"An alternative to sampling with temperature, called nucleus sampling,  where the model considers the results of the tokens with top_p probability mass. So 0.1 means only the tokens comprising the top 10% probability mass are considered.","displayCondition":null,"expressionEnabled":null,"hidden":null,"metadata":{},"required":false,"name":"topP","type":"NUMBER","defaultValue":1.0,"exampleValue":null,"label":"Top p","placeholder":null,"maxNumberPrecision":null,"maxValue":null,"minNumberPrecision":null,"minValue":null,"numberPrecision":null,"options":null,"optionsDataSource":null,"controlType":"NUMBER"},{"advancedOption":null,"description":"Number between -2.0 and 2.0. Positive values penalize new tokens based on their existing frequency in the text so far, decreasing the model's likelihood to repeat the same line verbatim.","displayCondition":null,"expressionEnabled":null,"hidden":null,"metadata":{},"required":false,"name":"frequencyPenalty","type":"NUMBER","defaultValue":0.0,"exampleValue":null,"label":"Frequency penalty","placeholder":null,"maxNumberPrecision":null,"maxValue":2.0,"minNumberPrecision":null,"minValue":-2.0,"numberPrecision":null,"options":null,"optionsDataSource":null,"controlType":"NUMBER"},{"advancedOption":null,"description":"Number between -2.0 and 2.0. Positive values penalize new tokens based on whether they appear in the text so far, increasing the model's likelihood to talk about new topics.","displayCondition":null,"expressionEnabled":null,"hidden":null,"metadata":{},"required":false,"name":"presencePenalty","type":"NUMBER","defaultValue":0.0,"exampleValue":null,"label":"Presence penalty","placeholder":null,"maxNumberPrecision":null,"maxValue":2.0,"minNumberPrecision":null,"minValue":-2.0,"numberPrecision":null,"options":null,"optionsDataSource":null,"controlType":"NUMBER"},{"advancedOption":null,"description":"Keeping the same seed would output the same response.","displayCondition":null,"expressionEnabled":null,"hidden":null,"metadata":{},"required":false,"name":"seed","type":"INTEGER","defaultValue":null,"exampleValue":null,"label":"Seed","placeholder":null,"maxValue":null,"minValue":null,"options":null,"optionsDataSource":null,"controlType":"INTEGER"}],"title":"Ask","output":null,"perform":{},"processErrorResponse":null,"outputResponse":{"outputSchema":{"advancedOption":null,"description":null,"displayCondition":null,"expressionEnabled":null,"hidden":null,"metadata":{},"required":null,"name":null,"type":"STRING","defaultValue":null,"exampleValue":null,"label":null,"placeholder":null,"controlType":"TEXT","languageId":null,"maxLength":null,"minLength":null,"options":null,"optionsDataSource":null},"sampleOutput":null},"workflowNodeDescription":null}],"connection":{"authorizations":[{"detectOn":null,"description":null,"name":"bearer_token","properties":[{"advancedOption":null,"description":"URL to your Ollama server","displayCondition":null,"expressionEnabled":null,"hidden":null,"metadata":{},"required":true,"name":"url","type":"STRING","defaultValue":null,"exampleValue":null,"label":"Url","placeholder":null,"controlType":"TEXT","languageId":null,"maxLength":null,"minLength":null,"options":null,"optionsDataSource":null}],"refreshOn":null,"title":"Bearer Token","type":"BEARER_TOKEN","acquire":null,"apply":null,"authorizationCallback":null,"authorizationUrl":null,"clientId":null,"clientSecret":null,"pkce":null,"refresh":null,"refreshUrl":null,"scopes":null,"refreshToken":null,"tokenUrl":null}],"properties":null,"version":1,"authorizationRequired":null,"baseUri":null,"test":null},"triggers":null}